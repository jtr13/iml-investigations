---
title: "Variable Importance Measure"
author: "Luying Shu"
execute:
  echo: true
format:
  html:
    fig-width: 6
    fig-height: 4
    out-width: 60%
    embed-resources: true
---

Greenwell, Brandon M., Bradley C. Boehmke, and Andrew J. McCarthy. “A Simple and Effective Model-Based Variable Importance Measure.” arXiv, May 12, 2018. https://doi.org/10.48550/arXiv.1805.04755.


## Background and Motivation

Many modern machine learning methods — such as **Random Forests (RF)**, **Gradient Boosting Machines (GBM)**, **Neural Networks (NN)**, and **Support Vector Machines (SVM)** — offer strong predictive performance but suffer from limited interpretability.

**Variable Importance (VI)** serves as a key tool to improve model interpretability. However, definitions of VI differ across models and are not universally comparable.

For example:

- **Random Forests (RF)** measure importance through node purity improvement or increases in out-of-bag (OOB) error.
- **Linear Regression** uses **t-values** to assess variable significance.
- **Neural Networks** often rely on methods like **Garson’s algorithm** or **Olden’s method**.

Partial Dependence Plots (PDPs) help visualize the relationship between individual features and model predictions, but until now, no systematic framework has leveraged PDPs to **quantify** variable importance.

## PDP Computation Process

1. **Select a variable** (e.g., `lstat`) and specify a sequence of values (e.g., 0, 5, 10, ..., 35).

2. **Set the selected variable to a fixed value** across all observations in the dataset (e.g., set all `lstat = 10`), while keeping other variables unchanged.

3. **Use the model to predict** the target variable (e.g., `medv` for house prices).

4. **Compute the mean prediction** across all observations — this represents the model’s average predicted outcome when the variable is fixed at this value.

5. **Repeat Steps 2–4** for all selected values of the variable.

6. **Plot the results**:
- **X-axis**: the values of the selected variable.
- **Y-axis**: the corresponding mean predictions.


```{r}
#install.packages("randomForest")
library(randomForest)
library(ISLR2) # for Boston dataset
library(ggplot2)
library(vip)
library(dplyr)
set.seed(150)
train <- sample(nrow(Boston), .98*nrow(Boston))
train_dat <- Boston[train,]
test_dat <- Boston[-train,]
bag.fit <- randomForest(
  medv ~ ., data = train_dat,
  mtry  = ncol(Boston) - 1,    # p = 12 predictors
  ntree = 500,
  importance = TRUE,           # store VI
  keep.inbag = TRUE
)
bag.fit
# ------------------------------------------------------------
# 1.  Build a PDP for every predictor in train_dat ------------
# ------------------------------------------------------------
vars <- setdiff(names(train_dat), "medv")     # all 12 predictors

pdp_list <- lapply(vars, function(v) {
  cat("→ computing PDP for", v, "\n")         # progress message
  pd <- pdp::partial(
    object            = bag.fit,
    pred.var          = v,
    train             = train_dat,
    grid.resolution   = 20,        # 20 grid points per variable
    progress          = "none"     # set to "text" if you like a progress bar
  )

  colnames(pd) <- c("value", "yhat")
  pd$Variable  <- v
  pd
})

plot_data <- bind_rows(pdp_list)

head(plot_data)

# ------------------------------------------------------------
# 2.  Faceted PDP plot ---------------------------------------
# ------------------------------------------------------------
ggplot(plot_data, aes(x = value, y = yhat)) +
  geom_line() +
  facet_wrap(~ Variable, scales = "free_x", ncol = 4) +
  labs(
    x     = "Predictor value",
    y     = "Predicted medv (PDP)",
    title = "Partial-dependence plots for bagged random-forest model"
  ) +
  theme_minimal()


```



Then we define variable importance based on the **variation (fluctuation amplitude) of PDP curves**.

- If a variable’s PDP is flat, it suggests little impact on predictions.
- If a variable’s PDP shows large fluctuations, it suggests a strong impact on predictions.


### Calculation Method

#### For Continuous Variables:
$$
VI(x_j) = \sqrt{\frac{1}{k-1} \sum_{i=1}^k \left(f_j(x_{ji}) - \frac{1}{k} \sum_{i=1}^k f_j(x_{ji})\right)^2}
$$
That is, the **standard deviation of the PDP** values.

#### For Categorical Variables:
$$
VI(x_j) = \frac{\max_i \bar{f}_j(x_{ji}) - \min_i \bar{f}_j(x_{ji})}{4}
$$
That is, the **range of PDP values divided by 4**.

In the example of Boston data

```{r}
library(purrr)
categorical_vars <- c("chas", "rad")

compute_vi_continuous <- function(df) {
  sd(df$yhat)
}

compute_vi_categorical <- function(df) {
  summary_df <- df |>
    group_by(value) |>
    summarise(mean_yhat = mean(yhat), .groups = "drop")
  (max(summary_df$mean_yhat) - min(summary_df$mean_yhat)) / 4
}

vi_pdp <- plot_data |>
  group_split(Variable) |>
  map_df(function(df) {
    varname <- unique(df$Variable)
    if (varname %in% categorical_vars) {
      vi <- compute_vi_categorical(df)
    } else {
      vi <- compute_vi_continuous(df)
    }
    tibble(Variable = varname, VI = vi)
  }) |>
  arrange(desc(VI))

print(vi_pdp)

```

```{r}
bag.fit$importance
```

## Applying the Same Method to Boosting

We now apply the same PDP-based variable importance method to a **boosting model (xgboost)** on the Boston dataset.

```{r}
library(xgboost)
library(Matrix)
library(pdp)

set.seed(256)

train_x <- as.matrix(train_dat[, -which(names(train_dat) == "medv")])
train_y <- train_dat$medv

dtrain <- xgb.DMatrix(data = train_x, label = train_y)

xgb.fit <- xgboost(
  data = dtrain,
  objective = "reg:squarederror",
  nrounds = 200,
  max_depth = 3,
  eta = 0.1,
  verbose = 0
)

# ------------------------------------------------------------
# 1. Build PDP for every predictor ---------------------------
# ------------------------------------------------------------
vars <- colnames(train_x)

pdp_list_boost <- lapply(vars, function(v) {
  cat("→ computing PDP for", v, "\n")
  pd <- pdp::partial(
    object = xgb.fit,
    pred.var = v,
    train = as.data.frame(train_x),
    grid.resolution = 20,
    progress = "none"
  )
  colnames(pd) <- c("value", "yhat")
  pd$Variable <- v
  pd
})

plot_data_boost <- bind_rows(pdp_list_boost)

# ------------------------------------------------------------
# 2. Faceted PDP plot for Boosting ----------------------------
# ------------------------------------------------------------
ggplot(plot_data_boost, aes(x = value, y = yhat)) +
  geom_line() +
  facet_wrap(~ Variable, scales = "free_x", ncol = 4) +
  labs(
    x = "Predictor value",
    y = "Predicted medv (PDP)",
    title = "Partial-dependence plots for boosting (xgboost) model"
  ) +
  theme_minimal()
```

```{r}
vi_pdp_boost <- plot_data_boost |>
  group_split(Variable) |>
  map_df(function(df) {
    varname <- unique(df$Variable)
    vi <- if (varname %in% categorical_vars) {
      compute_vi_categorical(df)
    } else {
      compute_vi_continuous(df)
    }
    tibble(Variable = varname, VI = vi)
  }) |>
  arrange(desc(VI))

print(vi_pdp_boost)
```

```{r}
xgb.model.dt.tree(model = xgb.fit)
# Returns details of all split nodes in each tree.
```

-------------------------------



In the context of tree-based models such as XGBoost, three commonly reported metrics for assessing variable importance are **Gain**, **Cover**, and **Frequency**. **Gain** measures the total improvement in the model’s objective function (typically a reduction in loss) brought by splits on a particular feature, making it the most direct indicator of a feature’s contribution to predictive accuracy. **Cover** reflects the number of data instances affected by splits on the feature, providing insight into how broadly the feature influences the data partitioning process. **Frequency** simply counts how often a feature is used for splitting across all trees, capturing the model’s preference for utilizing the feature, though it does not directly reflect predictive power. Among these, **Gain** is generally regarded as the most reliable measure for evaluating a feature’s importance, while **Cover** and **Frequency** offer complementary perspectives on the breadth and frequency of feature usage within the model.

```{r}
xgb.importance(model = xgb.fit)
```

For a tree model, a data.table with the following columns:

### 1. Gain

**Definition:**  
Gain measures how much a particular feature improves the model’s accuracy (reduces the loss) when it is used to split the data. It reflects the **contribution of this feature to reducing the loss function**.

**How it is calculated:**
- For each split in each tree where the feature is used, XGBoost calculates the reduction in loss (using second-order gradient information).
- The total Gain is the **sum of all such reductions** across all trees and splits where this feature is used.
- Finally, the Gain values are typically normalized to sum to 1.

$$
\text{Gain}_j = \frac{\text{Total reduction in loss when splitting on feature } j}{\text{Total reduction in loss for all features}}
$$

**Normalization formula:**
$$
\text{Normalized Gain}_j = \frac{\text{Raw Gain}_j}{\sum_{k=1}^p \text{Raw Gain}_k}
$$

**Interpretation:** A higher Gain means this feature contributes more to reducing error.


---

### 2. Cover

**Definition:**  
Cover measures the **number of data instances affected by splits on this feature**. It reflects how often and how many samples this feature helps to partition.

**How it is calculated:**
- For each split, XGBoost records the number of samples (or Hessian weight sum) at the node.
- The Cover is the **sum of these sample counts for all splits on this feature**.
- The Cover values are normalized to sum to 1.

$$
\text{Cover}_j = \frac{\text{Total number of samples covered by splits on feature } j}{\text{Total number of samples across all splits}}
$$

**Normalization formula:**
$$
\text{Normalized Cover}_j = \frac{\text{Raw Cover}_j}{\sum_{k=1}^p \text{Raw Cover}_k}
$$

**Interpretation:** A higher Cover means this feature splits on nodes that cover a larger portion of the data.


---

### 3. Frequency

**Definition:**  
Frequency measures **how often a feature is used to split** the data across all trees.

**How it is calculated:**
- Count how many times this feature is used in splits across all trees.
- Normalize by the total number of splits across all features.

$$
\text{Frequency}_j = \frac{\text{Number of splits using feature } j}{\text{Total number of splits across all features}}
$$

**Normalization formula:**
$$
\text{Normalized Frequency}_j = \frac{\text{Raw Frequency}_j}{\sum_{k=1}^p \text{Raw Frequency}_k}
$$

**Interpretation:** A higher Frequency means the feature is used more often, though this does not always indicate higher predictive power.

```{r}
library(xgboost)
library(dplyr)

# Extract tree structure
tree_dt <- xgb.model.dt.tree(model = xgb.fit)

# Exclude leaf nodes
split_dt <- tree_dt |>
  filter(Feature != "Leaf")

# Calculate raw Frequency, Gain, Cover
frequency <- split_dt |>
  group_by(Feature) |>
  summarise(Frequency = n())

gain <- split_dt |>
  group_by(Feature) |>
  summarise(Gain = sum(Quality))

cover <- split_dt |>
  group_by(Feature) |>
  summarise(Cover = sum(Cover))

# Merge
importance_manual <- frequency |>
  left_join(gain, by = "Feature") |>
  left_join(cover, by = "Feature")

# Normalize to sum to 1
importance_manual <- importance_manual |>
  mutate(
    Gain = Gain / sum(Gain),
    Cover = Cover / sum(Cover),
    Frequency = Frequency / sum(Frequency)
  ) |>
  arrange(desc(Gain))

print(importance_manual)


```
```{r}
xgb.importance(model = xgb.fit)
```

```{r}
print(vi_pdp_boost)
```
