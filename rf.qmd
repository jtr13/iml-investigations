---
title: "Random Forest"
author: "Luying Shu"
execute:
  echo: true
format:
  html:
    fig-width: 6
    fig-height: 4
    out-width: 60%
    embed-resources: true
---


# The Revolution of Ensemble Methods (1990s–2000s): Decision Trees as the Foundation

## Bagging (Bootstrap Aggregating)

**Work:**  
Breiman, L. (1996). *Bagging Predictors*. *Machine Learning*, *24*(2), 123–140.

**Contribution:**  
Bagging builds multiple models (typically decision trees) by training each on a different bootstrapped sample of the training data. The final prediction is made via **majority voting** (for classification) or **averaging** (for regression). This method **significantly reduces variance**, increasing the **stability** and **generalization** of the model. Bagging laid the groundwork for Random Forests.

**Key Concepts:**
- Bootstrap sampling  
- Independent model training  
- Voting/Averaging for final prediction  
- Variance reduction and improved robustness

---

## Random Forest

**Work:**  
Breiman, L. (2001). *Random Forests*. *Machine Learning*, *45*(1), 5–32.

**Contribution:**  
An extension of Bagging that introduces **feature randomness**. During tree construction, not only are the training samples bootstrapped, but at each node, a **random subset of features** is selected to determine the best split. This further **reduces correlation** between trees and enhances the model’s **accuracy**, **robustness**, and **resistance to overfitting**. It has become one of the most powerful and widely used machine learning algorithms.

**Key Innovations:**
- Sample bootstrapping (as in Bagging)  
- Random feature selection at each split  
- De-correlation of trees  
- High predictive performance and generalization


```{r}
#install.packages("randomForest")
library(randomForest)
library(ISLR2) # for Boston dataset
library(ggplot2)
library(vip)
set.seed(150)
train <- sample(nrow(Boston), .98*nrow(Boston))
train_dat <- Boston[train,]
test_dat <- Boston[-train,]
bag.boston <- randomForest(medv ~ ., data = train_dat, mtry = 12)
bag.boston
head(Boston)
```

```{r}
## Bagging ≈ RF with mtry = p
bag.fit <- randomForest(
  medv ~ ., data = train_dat,
  mtry  = ncol(Boston) - 1,    # p = 12 predictors
  ntree = 500,
  importance = TRUE,           # store VI
  keep.inbag = TRUE
)

bag.fit                            # OOB error & VI printed

```




```{r}
# 2.  OUT-OF-SAMPLE PERFORMANCE
pred_test <- predict(bag.fit, newdata = test_dat)
rmse_test <- sqrt(mean((pred_test - test_dat$medv) ^ 2))
rmse_test
# A baseline lm for reference
rmse_lm <- sqrt(mean((predict(lm(medv ~ ., train_dat), test_dat) - test_dat$medv)^2))

sprintf("Test-set RMSE: Bagging = %.2f ;  OLS = %.2f", rmse_test, rmse_lm)

```


$$
\text{RMSE} = \sqrt{ \frac{1}{n} \sum_{i=1}^{n} \left( \hat{y}_i - y_i \right)^2 }
$$
rmse_test ≈ 4.18. This indicates that the Bagging model achieves an RMSE of approximately 4.18 on the test set. rmse_lm ≈ 5.73. The linear model produces a higher RMSE of 5.73, indicating lower predictive accuracy.

Bagging outperforms OLS, achieving lower prediction error on unseen data. This demonstrates Bagging’s ability to reduce variance and model complex relationships that a linear model may fail to capture. Since RMSE penalizes large errors, a lower RMSE indicates better generalization.

### randomForest: importance, importanceSD, localImp(Regression)

```{r}
bag.fit <- randomForest(
  medv ~ ., data = train_dat,
  mtry  = ncol(Boston) - 1,    # p = 12 predictors
  ntree = 500,
  importance = TRUE,           # store VI
  importanceSD = TRUE,
  keep.inbag = TRUE,
  localImp = TRUE
)
```

```{r}
bag.fit$importance

```

For Regression, the first column is the mean decrease in accuracy and the second the mean decrease in MSE. 

%IncMSE — Percent Increase in Mean Squared Error - **train_dat(OOB part)**

This metric shows how much the model's prediction error (MSE) increases when a variable is randomly permuted, evaluated using the Out-of-Bag (OOB) samples. It reflects how important a variable is to the model’s predictive power.

How it is calculated(explained in the following section Variable Importacne(Regression)).


IncNodePurity — Increase in Node Purity - **train_dat**


This metric is based on the reduction in residual sum of squares (RSS) whenever a variable is used to split a node in the decision trees. It represents the total improvement in node purity across all splits using that variable.

How it is calculated:
1. Residual Sum of Squares (RSS) at a Node(All node split records are computed based on the entire train_dat dataset)
For any node $t$ in a regression tree, the **RSS** is defined as:

$$
RSS_t = \sum_{i \in t} (y_i - \bar{y}_t)^2
$$

Where:  
- $y_i$ is the response value of observation $i$ within node $t$  
- $\bar{y}_t$ is the mean response within node $t$

2. Purity Improvement at a Split
When node $t$ is split into left and right child nodes $t_L$ and $t_R$, the reduction in RSS is:

$$
\Delta RSS = RSS_t - (RSS_{t_L} + RSS_{t_R})
$$

3. Cumulative IncNodePurity for a Variable
For a given variable $X_j$, the **Increase in Node Purity** is the cumulative reduction in RSS across all nodes where $X_j$ is used to split:

$$
\text{IncNodePurity}(X_j) = \sum_{k: \text{split on } X_j} \Delta RSS_k
$$


```{r}
View(bag.fit$localImp)
```

Permutation Importance based on Sample-level MSE Change (localImp):
Instead of computing the average increase in mean squared error (MSE) across all out-of-bag (OOB) samples, localImp records the impact on the prediction error for each individual OOB observation when a specific variable is permuted. This allows for a more fine-grained, sample-level understanding of variable importance.

For each OOB sample and for each variable, the increase (or decrease) in prediction error after permuting that variable is recorded.

How it is calculated:

For each observation $i$ and each variable $j$:

1. Original Error (Before Permutation)  
Compute the squared error on OOB observation $i$ without permuting variable $j$:

$$
\text{Error}_{\text{orig},i} = (y_i - \hat{y}_{\text{orig},i})^2
$$

2. Permuted Error (After Permutation) 
Randomly permute variable $j$ among the OOB samples, make a new prediction on observation $i$, and compute the squared error:

$$
\text{Error}_{\text{perm}(j),i} = (y_i - \hat{y}_{\text{perm}(j),i})^2
$$

3. Difference (Local Importance for Observation $i$, Variable $j$):

$$
\text{localImp}_{j,i} = \text{Error}_{\text{perm}(j),i} - \text{Error}_{\text{orig},i}
$$

In the final result, each row shows the local importance scores for this variable across different observations, each column shows how important each variable is to predicting this particular observation(OOB samples).


- **Positive value**: Permuting variable $j$ increases the prediction error for observation $i$, indicating that $j$ is important for this sample.
- **Negative value**: Permuting variable $j$ decreases the prediction error for observation $i$, suggesting the variable might be noise or introduce instability for this sample.
- **Near zero**: Variable $j$ has little effect on this sample’s prediction.

```{r}
localimp <- bag.fit$localImp
rowavg <- rowMeans(localimp)
rowavg
# %IncMSE
# distribution
# localImp hist by Variable(...) 
# SD
```

**Difference between bag.fit$importance & rowMeans(bag.fit$localImp)**

The **`importance`** is the **global variable importance measure** directly provided by the `randomForest` package.
The **`rowMeans(localImp)`** is the **manually calculated average of the local variable importance across all samples**.

```{r}

library(ggplot2)
library(tidyr)
library(dplyr)

global_importance <- as.data.frame(bag.fit$importance)
local_avg <- rowMeans(localimp)

# Combine into one data frame
combined_importance <- data.frame(
  Variable = rownames(global_importance),
  `%IncMSE` = global_importance[, "%IncMSE"],
  LocalMeanImportance = local_avg,
  check.names = FALSE
) |> arrange(desc(`%IncMSE`))

print(combined_importance)

df_long <- combined_importance %>%
  pivot_longer(cols = c("%IncMSE", "LocalMeanImportance"),
               names_to = "ImportanceType",
               values_to = "Importance")

ggplot(df_long, aes(x = reorder(Variable, -Importance), y = Importance, fill = ImportanceType)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(
    x = "Variable",
    y = "Importance",
    title = "Comparison of %IncMSE and Local Mean Importance"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```



##### Nodepurity


**Local Importance Distributions Across Predictors**


```{r}
library(ggplot2)
library(tidyr)

local_imp_df <- as.data.frame(localimp)
local_imp_df$Variable <- rownames(local_imp_df)
local_imp_long <- pivot_longer(local_imp_df, 
                                cols = -Variable,
                                names_to = "Observation",
                                values_to = "Importance")

ggplot(local_imp_long, aes(x = Variable, y = Importance)) +
  geom_boxplot(fill = "skyblue") +
  labs(
    x = "Variable",
    y = "Local Importance (localImp)",
    title = "Distribution of Local Importance for Each Variable"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

The boxplot of local importance (localImp) values provides further insights into how each predictor contributes to the random forest model’s predictions at the observation level. Variables such as **lstat** and **rm** exhibit substantial variability in their local importance, with a wide range of values and numerous extreme outliers. This indicates that these variables have a strong and heterogeneous impact on the model’s predictions across different observations. Specifically, **lstat** shows the highest dispersion, suggesting that it plays a particularly crucial role for certain subsets of the data. In contrast, variables like **chas**, **zn**, and **rad** display local importance values consistently close to zero, implying that they have minimal influence on predictions for most observations. These findings are consistent with the global variable importance measures, where **lstat** and **rm** also rank highest in terms of %IncMSE. Overall, the distribution of local importance highlights that while some variables have a uniformly weak effect, others exert substantial influence in a highly observation-specific manner, aligning with the heterogeneous nature of housing price determinants in the Boston dataset.

```{r}
bag.fit$importance
bag.fit$importanceSD
```

The variable bag.fit$importanceSD contains the standard deviations of the %IncMSE variable importance measure across all trees for each predictor in the random forest model. It quantifies the variability (or uncertainty) of the %IncMSE values across trees.

Relationship:

bag.fit$importance provides the mean %IncMSE across trees (i.e., the averaged increase in MSE when permuting a variable).

bag.fit$importanceSD provides the standard deviation of %IncMSE across trees, reflecting how consistent (or unstable) the importance of that variable is from tree to tree.

Calculation Process: 

1. Out-of-Bag (OOB) Samples for Each Tree
For each tree in the Random Forest, there are some observations not used to train that tree (the OOB samples).

2. Permutation on OOB Samples
For each predictor (variable), permute (randomly shuffle) its values among the OOB samples. Measure the increase in the tree’s prediction error (MSE) after this permutation compared to the original OOB MSE.

3. Per-Tree %IncMSE
For each variable and for each tree, calculate the percentage increase in OOB MSE caused by permuting that variable:

$$
\%\text{IncMSE}_{j}^{(t)} = \frac{\text{MSE}_{\text{perm}(j)}^{(t)} - \text{MSE}_{\text{orig}}^{(t)}}{\text{MSE}_{\text{orig}}^{(t)}} \times 100
$$

Where:
- $t$: index for the tree
- $j$: index for the variable
- $\text{MSE}_{\text{orig}}$: OOB MSE without permutation
- $\text{MSE}_{\text{perm}}$: OOB MSE after permutation of variable $j$

4. Aggregate Across Trees
For each variable $j$, collect a set of $\%\text{IncMSE}_{j}^{(1)}, \%\text{IncMSE}_{j}^{(2)}, \ldots, \%\text{IncMSE}_{j}^{(T)}$, where $T$ is the total number of trees.

5. Compute Mean and Standard Deviation
- **Mean** across trees → stored in `bag.fit$importance`
- **Standard Deviation** across trees → stored in `bag.fit$importanceSD`

$$
\text{SD}(\%\text{IncMSE}_j) = \sqrt{\frac{1}{T-1} \sum_{t=1}^T \left( \%\text{IncMSE}_j^{(t)} - \overline{\%\text{IncMSE}_j} \right)^2}
$$

#### SD,localImp
```{r}
impsd <- bag.fit$importanceSD
local_sd <- apply(localimp, 1, sd, na.rm = TRUE)
# mean(localImp) as baseline MSE
local_mean <- apply(localimp, 1, function(x) mean(abs(x), na.rm = TRUE))
local_sd_percent <- local_sd / local_mean  # 转为“相对变异程度（%）”

local_sd_percent
impsd
```

### Interpretation of `local_sd_percent`

The `local_sd_percent` metric measures the **relative variability** of a variable’s local importance across individual observations. It is calculated as:

$$
\text{local_sd_percent}_j = \frac{\text{SD}(\text{localImp}_j)}{\text{Mean}(|\text{localImp}_j|)}
$$

Where:
- $\text{SD}(\text{localImp}_j)$ is the sample-wise standard deviation of variable $j$'s local importance values(for each row).
- $\text{Mean}(|\text{localImp}_j|)$ is the average absolute importance of variable $j$ across all OOB samples.

This normalized ratio allows us to understand **how consistently** a variable influences different observations.  
- A **higher** `local_sd_percent` means the variable has **heterogeneous local effects**, impacting some observations strongly and others weakly.
- A **lower** value suggests **uniform and stable importance** across samples.

It complements `importanceSD` (tree-wise standard deviation), offering insight into **local vs. global variability** of predictor effects.

```{r}
sd_df <- data.frame(
  Variable = names(impsd),
  importanceSD = as.vector(impsd),
  localSD_percent = local_sd_percent[names(impsd)]
)

sd_df$Variable <- factor(sd_df$Variable,
                         levels = sd_df$Variable[order(sd_df$importanceSD, decreasing = TRUE)])

sd_long <- pivot_longer(sd_df,
                        cols = c("importanceSD", "localSD_percent"),
                        names_to = "Type",
                        values_to = "SD")

library(ggplot2)
ggplot(sd_long, aes(x = Variable, y = SD, fill = Type)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(
    title = "Comparison of Tree-wise vs Relative Sample-wise Importance Variability",
    x = "Variable",
    y = "Standard Deviation (or Relative SD)",
    fill = "Metric Type"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

From the above bar chart, we observe clear differences in importance variability across metrics:

- Variables such as **`lstat`**, **`rm`**, and **`dis`** exhibit high values for both `importanceSD` and `localSD_percent`, suggesting that they are consistently influential across both trees and individual samples.
- On the contrary, variables like **`crim`**, **`ptratio`**, and **`rad`** show low `importanceSD` but relatively high `localSD_percent`, indicating that while they may not appear globally important, they exert **strong influence in specific subsets of data** — a signature of localized or context-specific importance.
- Interestingly, **`chas`** and **`zn`** present **negligible global importance** yet have **among the highest sample-wise variability**, implying they may be critical in rare or boundary cases, despite being globally inconsequential.
- Features such as **`nox`**, **`tax`**, and **`age`** demonstrate moderate and consistent importance in both global and local contexts.

While global metrics like `importanceSD` are essential for understanding **overall feature robustness**, they can overlook **individual-level predictive dynamics**. The `localSD_percent` serves as a complementary diagnostic, uncovering variables that contribute significantly to error reduction **in heterogeneous or edge-case samples**.



### Variable Importacne(Regression)

Permutation Importance based on MSE Increase:
Randomly shuffle the values of a specific variable (on the out-of-bag data) and compare the increase in model prediction error (MSE) to assess the importance of the variable.

In Breiman's book, the importance of the variable is measured by the v alue of “percentage of increase of mean square error” (Increase in MSE(%)) in Random Forest, where a higher MSE% value means a more important variable. 

The formula is as follows:

$$
\%\text{IncMSE}_j = \frac{ \text{MSE}_{\text{perm}(j)} - \text{MSE}_{\text{orig}} }{ \text{MSE}_{\text{orig}} } \times 100
$$

Where:

- $\text{MSE}_{\text{orig}}$: Original OOB MSE without permutation  
- $\text{MSE}_{\text{perm}(j)}$: OOB MSE after permuting variable $j$

A higher %IncMSE means that permuting the variable causes a larger increase in error, indicating higher importance.


```{r}
# Load required libraries
library(randomForest)
library(ISLR2)  # for Boston dataset
library(vip)    # for variable importance plots
library(dplyr)
library(ggplot2)

# Set seed for reproducibility
set.seed(150)

# Prepare data
train <- sample(nrow(Boston), .98*nrow(Boston))
train_dat <- Boston[train,]
test_dat <- Boston[-train,]

# Fit the bagging model (Random Forest with mtry = p)
bag.fit <- randomForest(
  medv ~ ., data = train_dat,
  mtry = ncol(Boston) - 1,    # p = 12 predictors (bagging)
  ntree = 500,
  importance = TRUE,          # store variable importance
  keep.inbag = TRUE
)

print("Original Random Forest Model:")
print(bag.fit)


# MANUAL CALCULATION OF VARIABLE IMPORTANCE (%IncMSE)
# Function to manually calculate %IncMSE for a given variable
calculate_manual_vi <- function(rf_model, train_data, variable_name) {
  
  # Step 1: Get original OOB predictions and calculate original MSE
  oob_pred_orig <- rf_model$predicted  # OOB predictions from original model
  y_true <- train_data$medv
  mse_orig <- mean((oob_pred_orig - y_true)^2)
  
  cat(sprintf("Original OOB MSE: %.4f\n", mse_orig))
  
  # Step 2: Create permuted dataset
  train_permuted <- train_data
  set.seed(123)  # for reproducibility of permutation
  train_permuted[[variable_name]] <- sample(train_permuted[[variable_name]])
  
  cat(sprintf("Permuting variable: %s\n", variable_name))
  
  # Step 3: Refit the model with permuted data
  rf_permuted <- randomForest(
    medv ~ ., data = train_permuted,
    mtry = ncol(Boston) - 1,
    ntree = 500,
    keep.inbag = TRUE
  )
  
  # Step 4: Get OOB predictions from permuted model and calculate new MSE
  oob_pred_perm <- rf_permuted$predicted
  mse_perm <- mean((oob_pred_perm - y_true)^2)
  
  cat(sprintf("Permuted OOB MSE: %.4f\n", mse_perm))
  
  # Step 5: Calculate %IncMSE
  percent_inc_mse <- (mse_perm - mse_orig) / mse_orig * 100
  
  cat(sprintf("Manual %%IncMSE for %s: %.4f\n", variable_name, percent_inc_mse))
  
  return(list(
    variable = variable_name,
    mse_orig = mse_orig,
    mse_perm = mse_perm,
    percent_inc_mse = percent_inc_mse
  ))
}

# CALCULATE FOR SPECIFIC VARIABLE (e.g., 'lstat')

cat("\n=== MANUAL CALCULATION FOR 'lstat' ===\n")
manual_vi_lstat <- calculate_manual_vi(bag.fit, train_dat, "lstat")


# COMPARE WITH BUILT-IN IMPORTANCE FUNCTION

cat("\n=== COMPARISON WITH BUILT-IN FUNCTIONS ===\n")

# Get built-in variable importance
vi_builtin <- importance(bag.fit, type = 1)  # type = 1 for %IncMSE
vi_lstat_builtin <- vi_builtin["lstat", "%IncMSE"]

cat(sprintf("Built-in %%IncMSE for lstat: %.4f\n", vi_lstat_builtin))
cat(sprintf("Manual %%IncMSE for lstat: %.4f\n", manual_vi_lstat$percent_inc_mse))
cat(sprintf("Difference: %.4f\n", abs(vi_lstat_builtin - manual_vi_lstat$percent_inc_mse)))


# CALCULATE FOR ALL VARIABLES (MANUAL vs BUILT-IN)

cat("\n=== CALCULATING FOR ALL VARIABLES ===\n")

# Get all variable names
all_vars <- names(train_dat)[names(train_dat) != "medv"]

# Manual calculation for all variables (this might take some time)
manual_vi_all <- data.frame(
  Variable = character(),
  Manual_IncMSE = numeric(),
  stringsAsFactors = FALSE
)

for (var in all_vars) {
  cat(sprintf("\nCalculating for %s...\n", var))
  manual_result <- calculate_manual_vi(bag.fit, train_dat, var)
  manual_vi_all <- rbind(manual_vi_all, 
                        data.frame(Variable = var, 
                                  Manual_IncMSE = manual_result$percent_inc_mse))
}

# Built-in importance for all variables
builtin_vi <- data.frame(
  Variable = rownames(vi_builtin),
  Builtin_IncMSE = vi_builtin[, "%IncMSE"]
)

# Combine results
comparison_df <- merge(manual_vi_all, builtin_vi, by = "Variable")
comparison_df$Difference <- abs(comparison_df$Manual_IncMSE - comparison_df$Builtin_IncMSE)
comparison_df <- comparison_df[order(-comparison_df$Builtin_IncMSE), ]

cat("\n=== COMPARISON TABLE ===\n")
print(comparison_df)


# VISUALIZATION

# Create comparison plot
comparison_long <- reshape2::melt(comparison_df[, c("Variable", "Manual_IncMSE", "Builtin_IncMSE")], 
                                 id.vars = "Variable")

p1 <- ggplot(comparison_long, aes(x = reorder(Variable, value), y = value, fill = variable)) +
  geom_bar(stat = "identity", position = "dodge") +
  coord_flip() +
  labs(title = "Manual vs Built-in Variable Importance (%IncMSE)",
       x = "Variables", y = "%IncMSE",
       fill = "Method") +
  scale_fill_manual(values = c("Manual_IncMSE" = "lightblue", "Builtin_IncMSE" = "coral"),
                    labels = c("Manual Calculation", "Built-in Function")) +
  theme_minimal()

print(p1)

# Plot using vip function for comparison
p2 <- vip(bag.fit, bar = TRUE) +
  ggtitle("Built-in Variable Importance (vip function)")

print(p2)


# SUMMARY STATISTICS

cat("\n=== SUMMARY STATISTICS ===\n")
cat(sprintf("Mean absolute difference: %.6f\n", mean(comparison_df$Difference)))
cat(sprintf("Max absolute difference: %.6f\n", max(comparison_df$Difference)))
cat(sprintf("Correlation between methods: %.6f\n", 
            cor(comparison_df$Manual_IncMSE, comparison_df$Builtin_IncMSE)))

# Show the most important variables from both methods
cat("\nTop 5 variables by built-in method:\n")
print(head(comparison_df[, c("Variable", "Builtin_IncMSE")], 5))

cat("\nTop 5 variables by manual method:\n")
print(head(comparison_df[order(-comparison_df$Manual_IncMSE), c("Variable", "Manual_IncMSE")], 5))
```

```{r}
vi(bag.fit, bar = TRUE)
```

Not the same, but the two results are similar.


```{r}
# 4.  GLOBAL EFFECTS – PARTIAL DEPENDENCE
library(pdp)
## single feature
pdp.rm <- pdp::partial(bag.fit, pred.var = "rm", train = train_dat)
ggplot()+
  geom_line(data = pdp.rm, aes(x=rm,y=yhat))+
  geom_point(data = pdp.rm, aes(x=rm,y=yhat), size = .5)+
  geom_rug(data = train_dat,aes(rm),color = "red", alpha = .3)
```
This code creates a Partial Dependence Plot that shows how the predicted response (medv, median home value) changes as the value of rm changes, while holding all other variables constant (marginalizing them out).

`rm` is clearly positively associated with house value.The relationship is nonlinear and saturates beyond a point (around 7.5–8 rooms).

If the PDP varies a lot in a certain interval but the rug is very small, it means that the data in that interval is sparse and the PDP results may be unstable or unreliable.

 The phenomenon observed in the figure: rm between 5 and 7 has the most data (rug density), and there are very few samples smaller than 4 or larger than 8.

Thus, the steep rise in PDP between 6.5 and 8 is plausible, while at the extremes (e.g., >8.5), PDP may be strongly influenced by small sample sizes.



```{r}
# ------------------------------------------------------------
# 1.  Build a PDP for every predictor in train_dat ------------
# ------------------------------------------------------------
vars <- setdiff(names(train_dat), "medv")     # all 12 predictors

pdp_list <- lapply(vars, function(v) {
  cat("→ computing PDP for", v, "\n")         # progress message
  pd <- pdp::partial(
    object            = bag.fit,
    pred.var          = v,
    train             = train_dat,
    grid.resolution   = 20,        # 20 grid points per variable
    progress          = "none"     # set to "text" if you like a progress bar
  )

  colnames(pd) <- c("value", "yhat")
  pd$Variable  <- v
  pd
})

plot_data <- bind_rows(pdp_list)

# ------------------------------------------------------------
# 2.  Faceted PDP plot ---------------------------------------
# ------------------------------------------------------------
ggplot(plot_data, aes(x = value, y = yhat)) +
  geom_line() +
  facet_wrap(~ Variable, scales = "free_x", ncol = 4) +
  labs(
    x     = "Predictor value",
    y     = "Predicted medv (PDP)",
    title = "Partial-dependence plots for bagged random-forest model"
  ) +
  theme_minimal()


```

```{r local-effects, message=FALSE, warning=FALSE}

library(pdp)       # partial(), ICE, PDP
library(dplyr)     # select, pipes
library(ggplot2)

# 1.  ICE curves for 'rm' 
ice_rm <- partial(
  bag.fit,
  pred.var        = "rm",
  train           = train_dat,
  ice             = TRUE,
  progress        = "none"
)

autoplot(ice_rm, alpha = 0.2) +
  ggtitle("ICE curves for rm (grey) + PDP (red)")

```

Gray Curve ICE Curve (Individual Conditional Expectation):

- For each observation in the training set, fix the remaining 11 features and just let the rm slide over a series of grid points and record the predictions, and then connect those points to a line. Each gray line = a “local response curve” for a house.

Red Curve PDP (Partial Dependence):

- The predicted values of all ICE curves averaged over the same rm points are connected to a line, indicating the marginal effect of the overall average.


### Variable Importacne(Classification)

Gini Importance based on Node Impurity Reduction (applicable only to classification trees):
Measures the total decrease in Gini impurity brought by a variable across all splits where it is used. The more a variable decreases impurity, the more important it is considered.

Let $p_{k}$ be the proportion of class $k$ observations in a node.  
The **Gini impurity** of a node is defined as:

$$G = 1 - \sum_{k=1}^{K} p_k^2$$

When a node is split on a variable, the decrease in impurity is calculated as:

$$\Delta G = G_{\text{parent}} - \left( \frac{n_{\text{left}}}{n_{\text{parent}}} G_{\text{left}} + \frac{n_{\text{right}}}{n_{\text{parent}}} G_{\text{right}} \right)$$

The total **Gini importance** for a variable is the sum of $\Delta G$ over all splits where this variable is used.


```{r}
library(ISLR2)
library(rpart)
library(tidyverse)
library(MASS)
# Fit a classification tree
kmod <- rpart(Kyphosis ~ Age + Number + Start,
              data = kyphosis,
              method = "class",
              control = rpart.control(cp = 0, minsplit = 2))

# View raw Gini importance from rpart
print(kmod$variable.importance)
```

Manual Calculation of Gini Importance

```{r}

# Access the frame of the tree
frame <- kmod$frame
node_ids <- as.numeric(rownames(frame))

# Identify internal (split) nodes
split_nodes <- node_ids[frame$var != "<leaf>"]

# Initialize importance list
gini_importance <- list()

if(length(split_nodes) > 0) {
  for (node in split_nodes) {
    # Current node info
    parent_row <- which(rownames(frame) == as.character(node))
    parent_n <- frame$n[parent_row]
    parent_dev <- frame$dev[parent_row]
    varname <- frame$var[parent_row]
    
    cat("\nProcessing node", node, "- Variable:", varname, "\n")
    
    # Left and right child node IDs
    left_id <- as.character(2 * node)
    right_id <- as.character(2 * node + 1)
    
    # Check if both children exist
    if (left_id %in% rownames(frame) && right_id %in% rownames(frame)) {
      left_row <- which(rownames(frame) == left_id)
      right_row <- which(rownames(frame) == right_id)
      
      left_n <- frame$n[left_row]
      right_n <- frame$n[right_row]
      left_dev <- frame$dev[left_row]
      right_dev <- frame$dev[right_row]
      
      # Calculate weighted impurity decrease
      # Note: frame$dev in rpart contains deviance, not Gini impurity directly
      # For classification trees, deviance = 2 * n * Gini_impurity
      # So Gini_impurity = deviance / (2 * n)
      
      parent_gini <- parent_dev / (2 * parent_n)
      left_gini <- left_dev / (2 * left_n)
      right_gini <- right_dev / (2 * right_n)
      
      # Weighted average of child impurities
      weighted_child_gini <- (left_n * left_gini + right_n * right_gini) / parent_n
      
      # Decrease in Gini impurity
      delta_gini <- parent_gini - weighted_child_gini
      
      # For comparison with rpart's importance, we need to multiply by sample size
      # This gives the total impurity decrease weighted by the number of samples
      importance_contribution <- delta_gini * parent_n
      
      cat("  Delta Gini:", delta_gini, "Contribution:", importance_contribution, "\n")
      
      # Accumulate importance
      if (varname %in% names(gini_importance)) {
        gini_importance[[varname]] <- gini_importance[[varname]] + importance_contribution
      } else {
        gini_importance[[varname]] <- importance_contribution
      }
    }
  }
} else {
  cat("No internal nodes found - tree has no splits\n")
}
# Convert to data frame and compare with rpart's output
if(length(gini_importance) > 0) {
  manual_importance <- unlist(gini_importance)
  
  # Create comparison table
  comparison_df <- data.frame(
    Variable = names(manual_importance),
    Manual_Gini_Importance = manual_importance,
    rpart_Gini_Importance = kmod$variable.importance[names(manual_importance)]
  )
  
  cat("\nComparison of Manual vs rpart Gini Importance:\n")
  print(comparison_df)
} else {
  cat("No importance calculated - no splits in tree\n")
}


```











